# Saliency Metrics

![Tests](https://github.com/valevalerio/SaliencyMetrics/actions/workflows/test.yml/badge.svg)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://badge.fury.io/py/SaliencyMetrics.svg)](https://badge.fury.io/py/SaliencyMetrics)
[![PyPI version](https://badge.fury.io/py/SaliencyMetrics.svg)](https://pypi.org/project/SaliencyMetrics/)
[![Documentation Status](https://readthedocs.org/projects/SaliencyMetrics/badge/?version=latest)](https://SaliencyMetrics.readthedocs.io/en/latest/?badge=latest)

This package implements the metrics used to compare different saliency maps generated by an explanation method. 
In order to have a fair comparison, the metrics should be computed on the same saliency map and ground truth map.

```tutorial.ipynb``` is an oiginal way used to check and test the different metrics. 
# Installation

```pip install SaliencyMetrics```

### This module is a work in progress and is not yet complete.

# Usage

```python
from SaliencyMetrics import ssim, psnr, emd

import numpy as np
import matplotlib.pyplot as plt



# create a random saliency map
saliency_map = np.random.rand(28*28).reshape(28, 28)
# create a random ground truth map
ground_truth_map = np.random.rand(28*28).reshape(28, 28)
# create a random binary mask

# use all the metrics to compare the saliency map with the ground truth map
for metric in [ssim, psnr, emd]:
    
    print(f"{metric.__name__}: {metric(saliency_map, ground_truth_map)}")
    

```
