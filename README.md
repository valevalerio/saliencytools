# Saliency Metrics

![Tests](https://github.com/valevalerio/SaliencyMetrics/actions/workflows/test.yml/badge.svg)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://img.shields.io/pypi/v/SaliencyMetrics)](https://pypi.org/project/SaliencyMetrics/)
![Documentation Status](https://readthedocs.org/projects/saliencymetrics/badge/?version=latest)

**Saliency Metrics** is a Python package that implements various metrics for comparing saliency maps generated by explanation methods. To ensure fair comparisons, metrics should be computed on the same saliency map and corresponding ground truth map.
The package includes the following metrics:
- **SSIM (Structural Similarity Index)**: A perceptual metric that quantifies the similarity between two images. It considers changes in structural information, luminance, and contrast.
- **PSNR (Peak Signal-to-Noise Ratio)**: A metric that measures the ratio between the maximum possible power of a signal and the power of corrupting noise. It is often used to assess the quality of reconstructed images.
- **EMD (Earth Mover's Distance)**: A metric that measures the distance between two probability distributions over a region D. It is often used in computer vision and image retrieval tasks.
```tutorial.ipynb``` is an oiginal way used to check and test the different metrics. 
# Installation

```pip install SaliencyMetrics```

### This module is a work in progress and is not yet complete.

# Usage

```python
from SaliencyMetrics import ssim, psnr, emd

import numpy as np
import matplotlib.pyplot as plt



# create a random saliency map
saliency_map = np.random.rand(28*28).reshape(28, 28)
# create a random ground truth map
ground_truth_map = np.random.rand(28*28).reshape(28, 28)
# create a random binary mask

# use all the metrics to compare the saliency map with the ground truth map
for metric in [ssim, psnr, emd]:
    
    print(f"{metric.__name__}: {metric(saliency_map, ground_truth_map)}")
    

```
